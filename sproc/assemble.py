# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/60_assemble.ipynb.

# %% auto 0
__all__ = ['merge_deleted', 'parquet_amenable', 'stack', 'distilled_data_from_zip', 'sparsity']

# %% ../nbs/60_assemble.ipynb 2
import pathlib

import pandas as pd

import sproc.structure
import sproc.bundle
import sproc.hier
import sproc.io
import sproc.postprocess
import sproc.parse

# %% ../nbs/60_assemble.ipynb 19
def merge_deleted(data_df: pd.DataFrame, deleted_series: pd.Series) -> pd.DataFrame:

    # if the `deleted_series` is empty
    if deleted_series.empty:

        res = data_df

        res['deleted_on'] = pd.NaT

        return res
    
    # duplicates are dropped (by means of `deduplicate_deleted_series`), so is the `file name` index, and the result turned into a DataFrame
    deduplicated_deleted_df = sproc.postprocess.deduplicate_deleted_series(deleted_series).droplevel(level=['zip', 'file name']).to_frame()
    
    # if this is a column-multiindexed dataframe
    if type(data_df.columns) == pd.MultiIndex:
    
        # in order to merge this new `pd.DataFrame` with `data_df` we need a *multiindex* for the former with the same number of levels as in the latter
        deduplicated_deleted_df.columns = pd.MultiIndex.from_tuples([sproc.hier.pad_col_levels(data_df, ['deleted_on'])])
    
    # the `data_df` is (*left*-)joined with the new one yielding deleted entries; the result is a dataframe in which a new (flag) column `deleted_on`
    # was added; notice that `data_df`'s index is reset for easying the merge (assuming every contract
    # shows only once in `data_df`, *id* should still provide a unique index...though it probably doesn't matter anyway)
    res = data_df.reset_index().set_index(['id']).merge(deduplicated_deleted_df, how='left', on=['id'])
    
    # on return, the index is left as it was
    return res.reset_index().set_index(['zip', 'file name', 'entry'])

# %% ../nbs/60_assemble.ipynb 31
def parquet_amenable(df: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:
    
    if inplace:
        
        res = df
        
    else:
        
        res = df.copy()
        
    # 1: every element in a multivalued column must be a list (maybe with a single element)
    res = sproc.io.homogenize_multivalued(res)
    
    # a list with the names of columns that are *multivalued*
    multivalued_columns = sproc.structure.multivalued_columns(res)
    
    # 2: the same type is enforced in all the `list`s (across rows and columns) in *multivalued* columns
    res[multivalued_columns] = res[multivalued_columns].applymap(sproc.io.cast_list_to_floats_or_strs)
    
    # 3: the same type for the elements in the list is enforced across every **individual** multivalued column
    res[multivalued_columns] = res[multivalued_columns].apply(sproc.io.cast_multivalued_series_to_common_type, axis='index')
    
    return res

# %% ../nbs/60_assemble.ipynb 49
def stack(top_df: pd.DataFrame, bottom_df: pd.DataFrame) -> pd.DataFrame:
    
    assert top_df.index.names == bottom_df.index.names, 'DataFrames are expected to have indexes with the same names'
    
    # if we have different number of levels in the dataframes...
    if top_df.columns.nlevels != bottom_df.columns.nlevels:
        
        if top_df.columns.nlevels < bottom_df.columns.nlevels:
            
            to_be_fixed_df = top_df
            
            res_n_levels = bottom_df.columns.nlevels
            
        else:
            
            to_be_fixed_df = bottom_df
            
            res_n_levels = top_df.columns.nlevels
            
        new_names = []
            
        for c in to_be_fixed_df.columns:
            
            new_names.append(c + ('',) * (res_n_levels - to_be_fixed_df.columns.nlevels))
            
        to_be_fixed_df.columns = pd.MultiIndex.from_tuples(new_names)
        
    return pd.concat((top_df, bottom_df), axis=0)

# %% ../nbs/60_assemble.ipynb 56
def distilled_data_from_zip(zip_file: pathlib.Path | str) -> tuple[pd.DataFrame, pd.Series]:
    
    # in case a `str` was passed
    zip_file = pathlib.Path(zip_file)
    
    # data is read from the above *zip* file (every file's data concatenated into a single `pd.DataFrame`)...
    df = sproc.bundle.read_zip(zip_file, concatenate=True)
    
    # ...and re-structured with multiindexed columns
    df = sproc.hier.flat_df_to_multiindexed_df(df)
    
    # the same contract might show up more than once due to updates...but only the last one is kept
    last_update_only_df = sproc.postprocess.keep_updates_only(df)
    
    # the same zip file also contains information (at the beginning) about deleted entries
    deleted_series = sproc.bundle.read_deleted_zip(zip_file)
    
    return last_update_only_df, deleted_series

# %% ../nbs/60_assemble.ipynb 66
def sparsity(df: pd.DataFrame, tidy_up: bool = False, do_not_modify_input: bool = True) -> pd.DataFrame:
    "Ratio of completeness for every (identified) administration"
    
    # a new `Domain` column is added
    df['domain'] = sproc.parse.domain(df)
    
    # it is used for grouping
    domain_grouped_df = df.groupby('domain', dropna=False)
    
    # for the sake of convenience
    domain_col = sproc.hier.pad_col_levels(df, ['domain'])
    
    # for every group we ascertain which elements (across every column) are not null (still a pd.DataFrame),
    # and then summarize it (one value per group and column) with mean (ratio of filled-in values)
    filled_in_ratio_df = domain_grouped_df.apply(
        lambda x: x.notnull().mean()).drop(domain_col, axis=1)
    
    if tidy_up:
        
        filled_in_ratio_df = filled_in_ratio_df.sort_index(axis=1).T
        
    if do_not_modify_input:
    
        # the added column is droppedsproc.postprocess.deduplicate_deleted_series(another_distilled_series)
        df.drop(domain_col, axis=1, inplace=True)
    
    return filled_in_ratio_df
