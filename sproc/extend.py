# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/70_extend.ipynb.

# %% auto 0
__all__ = ['df_with_zip', 'parquet_with_zip']

# %% ../nbs/70_extend.ipynb 2
import pathlib

import pandas as pd
import yaml

import sproc.structure
import sproc.bundle
import sproc.hier
import sproc.io
import sproc.assemble
import sproc.postprocess

# %% ../nbs/70_extend.ipynb 67
def df_with_zip(
    history_df: pd.DataFrame, # DataFrame to be extended
    zip_file: str | pathlib.Path, # Zip file with new data
    output_file: str | pathlib.Path | None = None # Output file (optional)
    ) -> None | pd.DataFrame: # Extended DataFrame or nothing if an `output_file` was passed
    "Extend an existing DataFrame with the data in a zip file"

    # in case `str`s were passed
    zip_file = pathlib.Path(zip_file)

    assert zip_file.exists(), f"can't find {zip_file}"
    
    # new data is parsed into a `pd.DataFrame`...
    new_df = sproc.bundle.read_zip(zip_file, concatenate=True)
    
    # ...whose columns are a *multiindex*
    new_df = sproc.hier.flat_df_to_multiindexed_df(new_df)
    
    # we also need the information about which entries are flagged as deleted
    deleted_series = sproc.bundle.read_deleted_zip(zip_file)
    
    # the time zone for the above deleted series and the column "deleted_on" in the historical data
    history_deleted_timezone = history_df["deleted_on"].dt.tz
    deleted_series_timezone = deleted_series.dt.tz
    
    # if there is any actual value in the "deleted on" column...
    if history_deleted_timezone:
    
        # assert deleted_series.dt.tz == history_deleted_timezone, f'{deleted_series.dt.tz=}, {history_deleted_timezone=}'
        
        if history_deleted_timezone != deleted_series_timezone:

            print(f'converting time zone in zip file {deleted_series_timezone} to {history_deleted_timezone} (that in the historical file)...')
            deleted_series = deleted_series.dt.tz_convert(history_deleted_timezone)
    
    # the two DataFrames are stacked together
    concatenated_df = sproc.assemble.stack(history_df, new_df)
    
    # the same contract might show up more than once due to updates...but only the last one is kept
    concatenated_df = sproc.postprocess.keep_updates_only(concatenated_df)
    
    # duplicates are dropped from the deleted series
    deduplicated_deleted_series = sproc.postprocess.deduplicate_deleted_series(deleted_series)
    
    # for the sake of flagging deleted entries, the new (concatenated) DataFrame and the "deleted series" are indexed the same
    reindexed_concatenated_df = concatenated_df.reset_index().set_index(['id'])
    # deduplicated_reindexed_deleted_series = deduplicated_deleted_series.droplevel(level='file name')
    deduplicated_reindexed_deleted_series = deduplicated_deleted_series.droplevel(level=['zip', 'file name'])
    
    # the `deleted_on` column in the DataFrame is filled in whenever appropriate
    reindexed_concatenated_df['deleted_on'] = reindexed_concatenated_df['deleted_on'].fillna(deduplicated_reindexed_deleted_series)
    
    # the original index is set back in place
    # res = reindexed_concatenated_df.reset_index().set_index(['file name', 'entry'])
    res = reindexed_concatenated_df.reset_index().set_index(['zip', 'file name', 'entry'])
    
    if output_file:
        
        # so that we can exploit pathlib's API...
        output_file = pathlib.Path(output_file)
        
        # ...for checking stuff
        assert output_file.suffix == '.parquet', f'output file should end in ".parquet"'
    
        parquet_df = sproc.assemble.parquet_amenable(res)
        
        parquet_df.to_parquet(output_file)
        
    else:
    
        return res

# %% ../nbs/70_extend.ipynb 69
def parquet_with_zip(
    history_file: str | pathlib.Path, # DataFrame to be extended
    zip_file: str | pathlib.Path, # Zip file with new data
    output_file: str | pathlib.Path | None = None # Output file (optional)
    ) -> None | pd.DataFrame: # Extended DataFrame or nothing if an `output_file` was passed
    "Extend an existing parquet file with the data in a zip file"
    
    # in case `str`s were passed
    history_file = pathlib.Path(history_file)
    
    assert history_file.exists(), f"can't find {history_file}"
    
    # historical data
    history_df = pd.read_parquet(history_file)

    return df_with_zip(history_df, zip_file, output_file)    
