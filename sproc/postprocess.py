# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/15_postprocess.ipynb.

# %% auto 0
__all__ = ['re_postal_zone', 'str_columns', 'assembled_str_columns', 'deadline_date_col', 'deadline_time_col',
           'deadline_datetime_col', 'status_col', 'historical_cols', 'assembled_historical_cols', 'typecast_columns',
           'keep_updates_only', 'deduplicate_deleted_series']

# %% ../nbs/15_postprocess.ipynb 2
import pathlib
import re

import pandas as pd
import numpy as np

import sproc.structure
import sproc.hier

# %% ../nbs/15_postprocess.ipynb 15
re_postal_zone = re.compile(sproc.structure.assemble_name(['.*', 'PostalZone$']))
re_postal_zone

# %% ../nbs/15_postprocess.ipynb 25
# str_columns = [['ContractFolderStatus', 'LocatedContractingParty', 'Party', 'PostalAddress', 'PostalZone']]
str_columns = []

# %% ../nbs/15_postprocess.ipynb 28
assembled_str_columns = [sproc.structure.assemble_name(c) for c in str_columns]

# %% ../nbs/15_postprocess.ipynb 31
deadline_date_col = sproc.structure.assemble_name(['ContractFolderStatus', 'TenderingProcess','TenderSubmissionDeadlinePeriod','EndDate'])
deadline_time_col = sproc.structure.assemble_name(['ContractFolderStatus', 'TenderingProcess','TenderSubmissionDeadlinePeriod','EndTime'])

# %% ../nbs/15_postprocess.ipynb 33
deadline_datetime_col = sproc.structure.assemble_name(['ContractFolderStatus', 'TenderingProcess', 'TenderSubmissionDeadlinePeriod'])

# %% ../nbs/15_postprocess.ipynb 35
status_col = sproc.structure.assemble_name(['ContractFolderStatus', 'ContractFolderStatusCode'])

# %% ../nbs/15_postprocess.ipynb 37
def typecast_columns(
    input_df: pd.DataFrame # Input DataFrame as ready by `to_df`
) -> pd.DataFrame: # Post-processed DataFrame
    "Tidy up the `pd.DataFrame` returned by `to_df`"
    
    res = input_df.copy()
    
    processed_columns = []
    
    # ------------ ContractFolderStatus - TenderingProcess - TenderSubmissionDeadlinePeriod ------------
    
    # only if they are present in the `pd.DataFrame`...
    if (deadline_date_col in res) and (deadline_time_col in res):
    
        # we don't want to inadvertently overwrite an existing column
        assert deadline_datetime_col not in res
    
        res[deadline_datetime_col] = pd.to_datetime(input_df[deadline_date_col] + 'T' + input_df[deadline_time_col], format='%Y-%m-%dT%H:%M:%S', utc=True, errors='coerce')   
    
        processed_columns.append(deadline_datetime_col)
    
    # NOTE: one could also delete the original columns being parsed, but they might be useful if errors happen during conversion (`errors=coerce`)
    
    # -------------------------------------------- updated ---------------------------------------------
    
    # after conversion, each date is wrapped into a list
    res['updated'] = pd.to_datetime(input_df['updated'], format='%Y-%m-%dT%H:%M:%S.%f%z', utc=True).apply(lambda x: [x])
    
    processed_columns.append('updated')
    
    # -------------------------------------------- status  ---------------------------------------------
    
    # it is wrapped into a list
    res[status_col] = res[status_col].apply(lambda x: [x])
    
    # ---------------------------------------- string columns ------------------------------------------
    
#     for c in assembled_str_columns:
        
#         if c in res:
        
#             res[c] = res[c].astype(pd.StringDtype())

#             processed_columns.append(c)
    
    # --------------------------------------- everything else ------------------------------------------
    
    for c in res.columns:
        
        # if this column has already been processed...
        if c in processed_columns:
            
            continue
            
        # if this column is *multivalued*...
        if sproc.structure.is_multivalued(res[c]):
            
            continue
            
        if re_postal_zone.match(c):
            
            res[c] = res[c].astype(pd.StringDtype())
            # print(f'converting {c}')
            
            continue
            
        # an attempt is made...
        try:
            
            # to interpret every column as a (float) number
            res[c] = res[c].astype(float)
        
        # if conversion to float is not feasible...
        except (TypeError, ValueError):
            
            # ...the column is taken to be one of strings
            res[c] = res[c].astype(pd.StringDtype())
    
    return res

# %% ../nbs/15_postprocess.ipynb 66
historical_cols = [['updated'], ['ContractFolderStatus', 'ContractFolderStatusCode']]
historical_cols

# %% ../nbs/15_postprocess.ipynb 68
assembled_historical_cols = [sproc.structure.assemble_name(c) for c in historical_cols]
assembled_historical_cols

# %% ../nbs/15_postprocess.ipynb 70
historical_cols = [['updated'], ['ContractFolderStatus', 'ContractFolderStatusCode']]

# %% ../nbs/15_postprocess.ipynb 72
def keep_updates_only(df: pd.DataFrame) -> pd.DataFrame:

#     grouped = df.sort_values('updated').groupby('id')
    
#     return grouped.tail(1)
    
    # if this is a column-multiindexed `pd.DataFrame`...
    if sproc.hier.is_column_multiindexed(df):
    
        cols = [sproc.hier.pad_col_levels(df, c) for c in historical_cols]
        
    # if this is NOT a column-multiindexed `pd.DataFrame`...
    else:
        
        cols = [sproc.structure.assemble_name(c) for c in historical_cols]
        
    # print(cols)
    
    # a `pd.DataFrame` capturing in a list all the values of the appointed columns for every group
    historical_df = df.sort_values('updated', key=np.vectorize(max)).groupby('id')[cols].agg(lambda x: x if len(x)==1 else np.concatenate(x.tolist()))
    
    # a `pd.DataFrame` just keeping the most recent update for every group
    no_duplicates_df = df.sort_values('updated', key=np.vectorize(max)).groupby('id').tail(1)
    
    assert len(no_duplicates_df) == len(historical_df)
    
    # they are merged together using `id`
    res = no_duplicates_df.drop(cols, axis=1).reset_index().set_index('id').merge(historical_df[cols], on='id').reset_index()
    
    # if the input `pd.DataFrame` is keeping the "source" as index...
    if df.index.names == ['zip', 'file name', 'entry']:
        
        res = res.set_index(['zip', 'file name', 'entry'])
    
    return res

# %% ../nbs/15_postprocess.ipynb 89
def deduplicate_deleted_series(series: pd.Series) -> pd.Series:
    
    return series.sort_values().groupby(axis=0, level='id', group_keys=False).nsmallest(1)
