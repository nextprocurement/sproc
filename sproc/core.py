# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['cli_process_zip', 'cli_extend_parquet_with_zip', 'cli_rename_columns', 'read_zips', 'cli_read_zips', 'update',
           'cli_update']

# %% ../nbs/00_core.ipynb 3
import sys
import argparse
import pathlib
import datetime

import yaml
import pandas as pd
from tqdm import tqdm

import sproc.extend
import sproc.hier
import sproc.assemble
import sproc.bundle
import sproc.postprocess
import sproc.structure
import sproc.download
import sproc.parse

# %% ../nbs/00_core.ipynb 7
def cli_process_zip(args: list = None) -> None:
    
    parser = argparse.ArgumentParser(description='Process zip file')

    parser.add_argument('zip_file', type=argparse.FileType('r'), help='zip file')
    parser.add_argument('output_file', help='Output (parquet) file')

    command_line_arguments = parser.parse_args(args)
    
    output_file = pathlib.Path(command_line_arguments.output_file)
    assert output_file.suffix == '.parquet', 'a .parquet file was expected'
    
    data_df, deleted_series = sproc.assemble.distilled_data_from_zip(command_line_arguments.zip_file.name)
    
    res = sproc.assemble.merge_deleted(data_df, deleted_series)
    res = sproc.assemble.parquet_amenable(res)
    
    res.to_parquet(output_file)

# %% ../nbs/00_core.ipynb 15
def cli_extend_parquet_with_zip(args: list = None) -> None:
    
    parser = argparse.ArgumentParser(description='Extend existing parquet file with data from a given zip')

    parser.add_argument('history_file', type=argparse.FileType('r'), help='Parquet file')
    parser.add_argument('zip_file', type=argparse.FileType('r'), help='Zip file')
    parser.add_argument('output_file', help='Output (parquet) file')

    command_line_arguments = parser.parse_args(args)
    
    history_file = pathlib.Path(command_line_arguments.history_file.name)
    zip_file = pathlib.Path(command_line_arguments.zip_file.name)
    
    output_file = pathlib.Path(command_line_arguments.output_file)
    assert output_file.suffix == '.parquet', 'a .parquet file was expected'
    
    sproc.extend.parquet_with_zip(history_file, zip_file, output_file)

# %% ../nbs/00_core.ipynb 23
def cli_rename_columns(args: list = None) -> None:
    
    parser = argparse.ArgumentParser(description='Rename columns')

    parser.add_argument('hierarchical_file', type=argparse.FileType('r'), help='(Hierarchical) Parquet file')
    parser.add_argument('mapping_file', type=argparse.FileType('r'), help='YAML file mapping hierarchical colum names to plain ones')
    parser.add_argument('output_file', help='Output (parquet) file')

    command_line_arguments = parser.parse_args(args)
    
    hierarchical_file = pathlib.Path(command_line_arguments.hierarchical_file.name)
    assert hierarchical_file.suffix == '.parquet', 'a (hierarchical) .parquet file was expected'
    
    mapping_file = pathlib.Path(command_line_arguments.mapping_file.name)
    assert (mapping_file.suffix == '.yaml') or (mapping_file.suffix == '.YAML'), 'a YAML file was expected'
    
    output_file = pathlib.Path(command_line_arguments.output_file)
    assert output_file.suffix == '.parquet', 'a .parquet file was expected'
    
    with mapping_file.open() as yaml_data:
        data_scheme = yaml.load(yaml_data, Loader=yaml.FullLoader)
        
    df = pd.read_parquet(hierarchical_file)
    renamed_cols_df = sproc.hier.flatten_columns_names(df, data_scheme)
    
    renamed_cols_df.to_parquet(output_file)

# %% ../nbs/00_core.ipynb 30
def read_zips(
    files: list[str | pathlib.Path] # Input files
    ) -> pd.DataFrame: # Procurement data
    "Build a `DataFrame` out of a bunch of zip files"
    
    # at the beginning it is guaranteed that every file is present
    for f in files:
        
        # in case `str` (rather than `Pathlib`s) were passed
        f = pathlib.Path(f)
        
        assert f.exists(), f'{f} doesn\'t exist'
    
    # accumulators for the data itself (contracts) and records of deleted entries
    res_df = None
    res_deleted_series = None

    for f in tqdm(files, desc='Assembling files'):
    # for f in files:

        # print(f'Processing "{f}"')
        tqdm.write(f'Processing "{f}"')

        # data is read from the above *zip* file, and `concatenate`d into a single `pd.DataFrame`...
        df = sproc.bundle.read_zip(f, concatenate=True)

        # ...which is re-structured with multiindexed columns
        df = sproc.hier.flat_df_to_multiindexed_df(df)

        # every ATOM inside the zip file also contains information (at the beginning) about deleted entries
        deleted_series = sproc.bundle.read_deleted_zip(f)

        # if this is NOT the first iteration...
        if res_df is not None:

            # ...the new data is stacked
            res_df = sproc.assemble.stack(res_df, df)
            res_deleted_series = pd.concat((res_deleted_series, deleted_series), axis=0)

        # ...if this is the first iteration
        else:

            # ...the new data is set as the accumulated result
            res_df = df
            res_deleted_series = deleted_series
            
    # some contracts show up more than once, and only the last update is to be kept
    res_last_update_only_df = sproc.postprocess.keep_updates_only(res_df)

    # a new *deleted* `pd.Series` is built by dropping duplicates (again, only the last one is kept)
    deduplicated_deleted_series = sproc.postprocess.deduplicate_deleted_series(res_deleted_series)

    # the *deleted* series is used to flag the appropriate entries in the "main" `pd.DataFrame`;
    # the result is "stateful" in the sense that we know the state of each entry (deleted -and, if so, when- or not)
    stateful_df = sproc.assemble.merge_deleted(res_last_update_only_df, deduplicated_deleted_series)
    
    # the number of filled-in rows for column `deleted_on` should match the number of `id`s in `deduplicated_deleted_series` that show up in `stateful_df`
    assert stateful_df['deleted_on'].notna().sum() == len(set(stateful_df['id']) & set(deduplicated_deleted_series.index.get_level_values(2)))
            
    return stateful_df

# %% ../nbs/00_core.ipynb 36
def cli_read_zips(args: list = None) -> None:
    
    parser = argparse.ArgumentParser(description='Process a bunch of zip files')

    parser.add_argument('input_files', type=argparse.FileType('r'), nargs='+', help='zip files')
    parser.add_argument('-o', '--output_file', default='out.parquet', help='Output (parquet) file')

    command_line_arguments = parser.parse_args(args)
    
    output_file = pathlib.Path(command_line_arguments.output_file)
    assert output_file.suffix == '.parquet', 'a .parquet file was expected'
        
    # the `pd.DataFrame` is built...
    df = read_zips([f.name for f in command_line_arguments.input_files])
    
    # ...rearranged for saving in parquet format
    parquet_df = sproc.assemble.parquet_amenable(df)
    
    parquet_df.to_parquet(output_file)
    
    print(f'writing {output_file}...')

# %% ../nbs/00_core.ipynb 40
def update(
    kind: str, # One of 'outsiders', 'insiders', or 'minors'
    output_directory: str | pathlib.Path # The path where hosting
    ):
    "Update local data structures"

    # `kind` should be one of the pre-set types
    assert kind in sproc.structure.tables

    # just in case
    output_directory = pathlib.Path(output_directory)

    # the output directory is expected to exist
    assert output_directory.exists()

    # the name of the output file is determined by `kind`, and it's a parquet file
    output_file = pathlib.Path(output_directory / kind).with_suffix('.parquet')

    # if a there is a previous file...
    if output_file.exists():

        print(f'found previous "{output_file}": extending it...')

        # the latter is read
        df = pd.read_parquet(output_file)

        # date strings are extracted from the "zip" index (level 0)...
        date_strs = df.index.get_level_values(0).drop_duplicates().str.extract('.*_([0-9]*).zip')[0].astype('str')

        # ...and parsed
        date_strs = date_strs.apply(sproc.parse.year_and_maybe_month)

        # the date from which to download new data is taken to be the maximum
        from_date = date_strs.max()

        # print(from_date)

        # print(sproc.download.make_urls(**sproc.structure.tables[kind], from_date=from_date))

        # required files are downloaded
        downloaded_files = sproc.download.from_date(kind, date=from_date, output_directory=output_directory)

        if not downloaded_files:

            print('file is up-to-date')

            return

        # in the beginning, the file to be updated represents the whole history
        history_df = df

        # every file that has been downloaded...
        for f in tqdm(downloaded_files, desc='Updatingtqdm'):

            tqdm.write(f'Appending "{f.name}"')

            # ...is used to extend the past
            history_df = sproc.extend.df_with_zip(history_df, f)

    # if a there is NOT a previous file...
    else:

        # agreed upon
        from_date = datetime.datetime(2017, 12, 1)

        print(f'no previous "{output_file}" was found: making one using data since {from_date.date()}...')

        # print(sproc.download.make_urls(**sproc.structure.tables[kind], from_date=from_date))

        # downloading
        downloaded_files = sproc.download.from_date(kind, date=from_date, output_directory=output_directory)

        # assembling
        history_df = read_zips(downloaded_files)

    # tidy up the `DataFrame` so that it can be saved in a parquet file
    parquet_df = sproc.assemble.parquet_amenable(history_df)
    
    # parquet_df.to_parquet(output_file.with_stem('new'))
    parquet_df.to_parquet(output_file)

# %% ../nbs/00_core.ipynb 43
def cli_update(args: list = None) -> None:
    
    parser = argparse.ArgumentParser(description='Update (or make) local data')

    parser.add_argument('kind', choices=sproc.structure.tables.keys())
    parser.add_argument('-o', '--output_directory', help='Output directory', default=pathlib.Path.cwd(), type=pathlib.Path)

    command_line_arguments = parser.parse_args(args)

    # directory is made if it doesn't exist
    command_line_arguments.output_directory.mkdir(exist_ok=True)

    print(command_line_arguments)

    update(command_line_arguments.kind, command_line_arguments.output_directory)
